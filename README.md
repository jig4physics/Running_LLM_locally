# Introduction 
This Repo contain Various way to runn LLM (or LM) in localsytem with/Withough GPU


## Suppoting installations 

# VLLM 
```
pip install nm-vllm[sparse]

```

# Citation 

[1] https://docs.vllm.ai/en/stable/