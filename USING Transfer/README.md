
# Text Generation with Meta Llama using Transformers

This project demonstrates how to use the `transformers` library by Hugging Face for text generation with the `Meta-Llama-3-8B` model.

## Features

- Simple setup for text generation tasks.
- Automatic device mapping for GPU/CPU optimization.
- Support for mixed precision (`bfloat16`) to improve performance.

---

## Installation

### Prerequisites

Ensure you have Python 3.8 or higher installed on your system.

### Install Required Libraries

Install the necessary libraries using pip:

```bash
pip install transformers torch
```

---

## Usage

1. **Load the Script**

   Save the script below as `llama_transformers.py`:

   ```python
   import transformers
   import torch

   # Specify the model ID
   model_id = "meta-llama/Meta-Llama-3-8B"

   # Initialize the pipeline
   pipeline = transformers.pipeline(
       "text-generation", 
       model=model_id, 
       model_kwargs={"torch_dtype": torch.bfloat16}, 
       device_map="auto"
   )

   # Generate text
   print(pipeline("Hey how are you doing today?"))
   ```

2. **Run the Script**

   Execute the script:

   ```bash
   python llama_transformers.py
   ```

3. **Output**

   The script will output a response generated by the `Meta-Llama-3-8B` model.

---

## Configuration Options

- **`model_id`**: Specify the ID of the model from Hugging Face's model hub.
- **`torch_dtype`**: Use mixed precision (`bfloat16`) for faster computation and reduced memory usage.
- **`device_map`**: Automatically maps the model across available GPUs/CPUs for optimal performance.

---

## Notes

- Ensure you have sufficient GPU memory to load the `Meta-Llama-3-8B` model.
- You may need to install additional libraries or drivers depending on your GPU.

---

## License

This project is licensed under the MIT License. Please refer to the `LICENSE` file for details.