
# Llama Model Prompt Interaction

This project demonstrates how to use the `llama_cpp` library to interact with a large language model (LLM) for generating completions. The example script prompts the model to name the planets in the solar system.

## Features

- Simple prompt-completion interaction.
- Configurable parameters such as context window, token limits, and stopping criteria.
- Supports optional GPU acceleration for faster inference.

---

## Installation

### Prerequisites

Ensure you have Python 3.8 or higher installed on your system.

### Install Required Libraries

Install the `llama_cpp` library:

```bash
pip install llama-cpp-python
```

---

## Usage

1. **Download the Model**

   Download the `llama-model.gguf` file and place it in the `./models/7B/` directory.

2. **Configure the Script**

   Update the `model_path` to point to the correct model file location.

3. **Run the Script**

   Save the script below as `llama_prompt.py`:

   ```python
   from llama_cpp import Llama

   # Initialize the model
   llm = Llama(
       model_path="./models/7B/llama-model.gguf",
       # n_gpu_layers=-1, # Uncomment to use GPU acceleration
       # seed=1337, # Uncomment to set a specific seed
       # n_ctx=2048, # Uncomment to increase the context window
   )

   # Generate output
   output = llm(
       "Q: Name the planets in the solar system? A: ", # Prompt
       max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window
       stop=["Q:", "\n"], # Stop generating just before the model would generate a new question
       echo=True # Echo the prompt back in the output
   ) # Generate a completion, can also call create_completion

   # Print the response
   print(output)
   ```

4. **Execute the Script**

   Run the script:

   ```bash
   python llama_prompt.py
   ```

5. **Output**

   The script will output the planets in the solar system as generated by the model.

---

## Configuration Options

- **`model_path`**: Specify the path to the model file.
- **`n_gpu_layers`**: Uncomment and set to `-1` to enable GPU acceleration.
- **`seed`**: Uncomment and set to a specific value to ensure reproducibility.
- **`n_ctx`**: Uncomment and set to increase the context window size.
- **`max_tokens`**: Specify the maximum number of tokens to generate.
- **`stop`**: Provide stop sequences to terminate generation at specific points.
- **`echo`**: Set to `True` to include the prompt in the output.

---

## License

This project is licensed under the MIT License. Please refer to the `LICENSE` file for details.